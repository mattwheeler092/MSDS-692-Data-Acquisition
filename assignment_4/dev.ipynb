{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloves = {}\n",
    "with open('data/glove.6B.300d.txt') as file:\n",
    "    for line in file.readlines():\n",
    "        elem = line.split()\n",
    "        gloves[elem[0]] = np.array([float(val) for val in elem[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    regex = \"[\" + string.punctuation + \"0-9\\\\r\\\\t\\\\n]\"\n",
    "    text = re.sub(regex, \" \", text.lower())\n",
    "    words = [w for w in text.split() if len(w) > 2]  \n",
    "    return [w for w in words if w not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doc2vec import get_text, filelist\n",
    "\n",
    "def load_articles(articles_dirname, gloves):\n",
    "    article_info = []\n",
    "    for file in filelist(articles_dirname):\n",
    "        title, *text = get_text(file).split('\\n')\n",
    "        text = '\\n'.join(text[1:-1])\n",
    "        word_vecs = np.array([gloves[word] for word in words(text) if word in gloves])\n",
    "        centroid = np.mean(word_vecs, axis=0)\n",
    "        article_info.append((file, title, text, centroid))\n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "def load_glove(filename):\n",
    "    \"\"\"\n",
    "    Read all lines from the indicated file and return a dictionary\n",
    "    mapping word:vector where vectors are of numpy `array` type.\n",
    "    GloVe file lines are of the form:\n",
    "\n",
    "    the 0.418 0.24968 -0.41242 0.1217 ...\n",
    "\n",
    "    So split each line on spaces into a list; the first element is the word\n",
    "    and the remaining elements represent factor components. The length of the vector\n",
    "    should not matter; read vectors of any length.\n",
    "    \"\"\"\n",
    "    gloves = {}\n",
    "    with open(filename) as file:\n",
    "        for line in file.readlines():\n",
    "            elem = line.split()\n",
    "            gloves[elem[0]] = np.array([float(val) for val in elem[1:]])\n",
    "    return gloves\n",
    "\n",
    "\n",
    "def filelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return allfiles\n",
    "\n",
    "\n",
    "def get_text(filename):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding=\"latin-1\", mode=\"r\")\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "\n",
    "def words(text):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    regex = \"[\" + string.punctuation + \"0-9\\\\r\\\\t\\\\n]\"\n",
    "    text = re.sub(regex, \" \", text.lower())\n",
    "    words = [w for w in text.split() if len(w) > 2]\n",
    "    return [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def doc2vec(text, gloves):\n",
    "    \"\"\"\n",
    "    Return the word vector centroid for the text. Sum the word vectors\n",
    "    for each word and then divide by the number of words. Ignore words\n",
    "    not in gloves.\n",
    "    \"\"\"\n",
    "    word_vecs = np.array([gloves[w] for w in words(text) if w in gloves])\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "\n",
    "def load_articles(articles_dirname, gloves):\n",
    "    \"\"\"\n",
    "    Load all .txt files under articles_dirname and return a table (list of lists/tuples)\n",
    "    where each record is a list of:\n",
    "\n",
    "      [filename, title, article-text-minus-title, wordvec-centroid-for-article-text]\n",
    "\n",
    "    We use gloves parameter to compute the word vectors and centroid.\n",
    "\n",
    "    The filename is fully-qualified name of the text file including\n",
    "    the path to the root of the corpus passed in on the command line.\n",
    "\n",
    "    When computing the vector for each document, use just the text, not the text and title.\n",
    "    \"\"\"\n",
    "    article_info = []\n",
    "    for file in filelist(articles_dirname):\n",
    "        if not file.endswith('README.TXT'):\n",
    "            title, *text = get_text(file).split(\"\\n\")\n",
    "            text = \"\\n\".join(text).strip(\"\\n\")\n",
    "            centroid = doc2vec(text, gloves)\n",
    "            article_info.append((file, title, text, centroid))\n",
    "    return article_info\n",
    "\n",
    "\n",
    "def distances(article, articles):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance from article to every other article and return\n",
    "    a list of (distance, a) tuples for all a in articles. The article is one\n",
    "    of the elements (tuple) from the articles list.\n",
    "    \"\"\"\n",
    "    target_filename, target_centroid = article[0], article[3]\n",
    "    distances_list = []\n",
    "    for article_info in articles:\n",
    "        filename, centroid = article_info[0], article_info[3]\n",
    "        if filename != target_filename:\n",
    "            distances_list.append((\n",
    "                np.linalg.norm(centroid - target_centroid),\n",
    "                article_info\n",
    "            ))\n",
    "    return distances_list\n",
    "\n",
    "\n",
    "def recommended(article, articles, n):\n",
    "    \"\"\"\n",
    "    Return a list of the n articles (records with filename, title, etc...)\n",
    "    closest to article's word vector centroid. The article is one of the elements\n",
    "    (tuple) from the articles list.\n",
    "    \"\"\"\n",
    "    distances_list = distances(article, articles)\n",
    "    return [elem[1] for elem in sorted(distances_list, key=lambda x: x[0])][:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloves = load_glove('data/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = load_articles('data/bbc', gloves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google launches TV search service'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1863][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data/bbc/tech/010.txt', \"Google's toolbar sparks concern\", 1.042),\n",
       " ('data/bbc/tech/011.txt', 'UK net users leading TV downloads', 1.054),\n",
       " ('data/bbc/tech/053.txt', 'Microsoft launches its own search', 1.104),\n",
       " ('data/bbc/tech/213.txt', 'Search sites get closer to users', 1.133),\n",
       " ('data/bbc/tech/267.txt', 'The year search became personal', 1.151),\n",
       " ('data/bbc/tech/108.txt', 'Search wars hit desktop PCs', 1.166),\n",
       " ('data/bbc/tech/269.txt', 'Yahoo moves into desktop search', 1.18),\n",
       " ('data/bbc/tech/186.txt', \"China 'blocks Google news site'\", 1.189),\n",
       " ('data/bbc/tech/394.txt', \"TV's future down the phone line\", 1.192),\n",
       " ('data/bbc/tech/216.txt', 'TV future in the hands of viewers', 1.2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_centroid = articles[1863][-1]\n",
    "comp_dist = lambda centroid: np.linalg.norm(centroid - original_centroid)\n",
    "\n",
    "[(x[0], x[1], round(comp_dist(x[-1]),3)) for x in recommended(articles[1863], articles, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863\n"
     ]
    }
   ],
   "source": [
    "for i, article in enumerate(articles):\n",
    "    if article[0] == 'data/bbc/tech/076.txt':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/bbc/entertainment/289.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
